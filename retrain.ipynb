{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c20634a-07bc-413b-bb72-7e983db93cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mnist_classifier.lenet import LeNet5, LeNet1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, ConcatDataset\n",
    "import random\n",
    "import os\n",
    "from  PIL import  Image\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms,utils,datasets\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import time\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e567f9b-844e-4687-be13-b7329b8fa6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_root = './autodl-tmp/mnist'\n",
    "mnist_train = MNIST(mnist_root,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize((32, 32)),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5,), (0.5,))]))\n",
    "mnist_test = MNIST(mnist_root,\n",
    "                  train=False,\n",
    "                  download=True,\n",
    "                  transform=transforms.Compose([\n",
    "                      transforms.Resize((32, 32)),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((0.5,), (0.5,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "059357d9-3d74-4288-9ecc-eeb59b558771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(Dataset):\n",
    "    def __init__(self,txt_path,train_flag=True):\n",
    "        self.tensors_info=self.get_tensors(txt_path)\n",
    "        self.train_flag=train_flag\n",
    "\n",
    "    def get_tensors(self, txt_path):\n",
    "        with open(txt_path,'r',encoding='UTF-8') as f:\n",
    "            tensors_info=f.readlines()\n",
    "            tensors_info=list(map(lambda x:x.strip().split('\\t'),tensors_info))\n",
    "        return tensors_info\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tensor_path,label=self.tensors_info[index]\n",
    "        try:\n",
    "            tensor = torch.load(tensor_path)\n",
    "        except:\n",
    "            print(tensor_path)\n",
    "        #返回打开的图片和它的标签\n",
    "        return tensor, int(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e5e4e09-d861-415b-8c7f-1d8786b62714",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_root = 'autodl-tmp/testcases/mnist/lenet5'\n",
    "\n",
    "train_set=LoadData(my_root + \"/train.txt\",True)\n",
    "test_set=LoadData(my_root + \"/test.txt\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a013c3dd-8f6f-4e21-88d9-dade90d4e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并数据集\n",
    "train_merged = ConcatDataset([mnist_train, train_set])\n",
    "train_merged_loader = DataLoader(train_merged, batch_size=256, shuffle=True, num_workers=8)\n",
    "\n",
    "test_merged = ConcatDataset([mnist_test, test_set])\n",
    "test_merged_loader = DataLoader(test_merged, batch_size=256, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ba9e2c8-511f-463b-9fab-7c55195fc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet5()\n",
    "# net.load_state_dict(torch.load('autodl-tmp/save_model/mnist/Lenet1/lenet_epoch=2_test_acc=0.816.pth'))\n",
    "net.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffc6a560-0250-43cc-baf3-11bedd3036f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global cur_batch_win\n",
    "    net.train()\n",
    "    loss_list, batch_list = [], []\n",
    "    for i, (images, labels) in enumerate(train_merged_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss_list.append(loss.detach().cpu().item())\n",
    "        batch_list.append(i+1)\n",
    "        if i % 10 == 0:\n",
    "             print('Train - Epoch %d, Batch: %d, Loss: %f' % (epoch, i, loss.detach().cpu().item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "238f212b-8338-434c-9010-01563b6fa39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        avg_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(test_merged_loader):\n",
    "            output = net(images)\n",
    "            avg_loss += criterion(output, labels).sum()\n",
    "            pred = output.detach().max(1)[1]\n",
    "            total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "\n",
    "    avg_loss /= len(test_merged)  # 除以测试集大小\n",
    "    acc = float(total_correct) / len(test_merged)\n",
    "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9aa3c7e9-5a1a-43f0-9d5f-8da1671f6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(epoch):\n",
    "    print('training...')\n",
    "    train(epoch)\n",
    "    acc = test()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "923c79a2-ff0c-42cb-8b61-d30fa9412b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Train - Epoch 1, Batch: 0, Loss: 2.303067\n",
      "Train - Epoch 1, Batch: 10, Loss: 2.295403\n",
      "Train - Epoch 1, Batch: 20, Loss: 2.283597\n",
      "Train - Epoch 1, Batch: 30, Loss: 2.271069\n",
      "Train - Epoch 1, Batch: 40, Loss: 2.259309\n",
      "Train - Epoch 1, Batch: 50, Loss: 2.225097\n",
      "Train - Epoch 1, Batch: 60, Loss: 2.195620\n",
      "Train - Epoch 1, Batch: 70, Loss: 2.136007\n",
      "Train - Epoch 1, Batch: 80, Loss: 2.098171\n",
      "Train - Epoch 1, Batch: 90, Loss: 2.052160\n",
      "Train - Epoch 1, Batch: 100, Loss: 1.955610\n",
      "Train - Epoch 1, Batch: 110, Loss: 1.804026\n",
      "Train - Epoch 1, Batch: 120, Loss: 1.715901\n",
      "Train - Epoch 1, Batch: 130, Loss: 1.634450\n",
      "Train - Epoch 1, Batch: 140, Loss: 1.418393\n",
      "Train - Epoch 1, Batch: 150, Loss: 1.282449\n",
      "Train - Epoch 1, Batch: 160, Loss: 1.177955\n",
      "Train - Epoch 1, Batch: 170, Loss: 1.026550\n",
      "Train - Epoch 1, Batch: 180, Loss: 0.964814\n",
      "Train - Epoch 1, Batch: 190, Loss: 0.880473\n",
      "Train - Epoch 1, Batch: 200, Loss: 0.843273\n",
      "Train - Epoch 1, Batch: 210, Loss: 0.808537\n",
      "Train - Epoch 1, Batch: 220, Loss: 0.720209\n",
      "Train - Epoch 1, Batch: 230, Loss: 0.745107\n",
      "Train - Epoch 1, Batch: 240, Loss: 0.502947\n",
      "Train - Epoch 1, Batch: 250, Loss: 0.684434\n",
      "Train - Epoch 1, Batch: 260, Loss: 0.663115\n",
      "Test Avg. Loss: 0.002149, Accuracy: 0.853682\n",
      "lenet_epoch=1_test_acc=0.854.pth\n",
      "training...\n",
      "Train - Epoch 2, Batch: 0, Loss: 0.535223\n",
      "Train - Epoch 2, Batch: 10, Loss: 0.501589\n",
      "Train - Epoch 2, Batch: 20, Loss: 0.566554\n",
      "Train - Epoch 2, Batch: 30, Loss: 0.553892\n",
      "Train - Epoch 2, Batch: 40, Loss: 0.428735\n",
      "Train - Epoch 2, Batch: 50, Loss: 0.478494\n",
      "Train - Epoch 2, Batch: 60, Loss: 0.426336\n",
      "Train - Epoch 2, Batch: 70, Loss: 0.434111\n",
      "Train - Epoch 2, Batch: 80, Loss: 0.530036\n",
      "Train - Epoch 2, Batch: 90, Loss: 0.427584\n",
      "Train - Epoch 2, Batch: 100, Loss: 0.437308\n",
      "Train - Epoch 2, Batch: 110, Loss: 0.408243\n",
      "Train - Epoch 2, Batch: 120, Loss: 0.331247\n",
      "Train - Epoch 2, Batch: 130, Loss: 0.397444\n",
      "Train - Epoch 2, Batch: 140, Loss: 0.337658\n",
      "Train - Epoch 2, Batch: 150, Loss: 0.393540\n",
      "Train - Epoch 2, Batch: 160, Loss: 0.332801\n",
      "Train - Epoch 2, Batch: 170, Loss: 0.384661\n",
      "Train - Epoch 2, Batch: 180, Loss: 0.349695\n",
      "Train - Epoch 2, Batch: 190, Loss: 0.369475\n",
      "Train - Epoch 2, Batch: 200, Loss: 0.316183\n",
      "Train - Epoch 2, Batch: 210, Loss: 0.287903\n",
      "Train - Epoch 2, Batch: 220, Loss: 0.306674\n",
      "Train - Epoch 2, Batch: 230, Loss: 0.303181\n",
      "Train - Epoch 2, Batch: 240, Loss: 0.242847\n",
      "Train - Epoch 2, Batch: 250, Loss: 0.255891\n",
      "Train - Epoch 2, Batch: 260, Loss: 0.448620\n",
      "Test Avg. Loss: 0.001145, Accuracy: 0.917527\n",
      "lenet_epoch=2_test_acc=0.918.pth\n",
      "training...\n",
      "Train - Epoch 3, Batch: 0, Loss: 0.272976\n",
      "Train - Epoch 3, Batch: 10, Loss: 0.324300\n",
      "Train - Epoch 3, Batch: 20, Loss: 0.301826\n",
      "Train - Epoch 3, Batch: 30, Loss: 0.236637\n",
      "Train - Epoch 3, Batch: 40, Loss: 0.330970\n",
      "Train - Epoch 3, Batch: 50, Loss: 0.226071\n",
      "Train - Epoch 3, Batch: 60, Loss: 0.225439\n",
      "Train - Epoch 3, Batch: 70, Loss: 0.375784\n",
      "Train - Epoch 3, Batch: 80, Loss: 0.277387\n",
      "Train - Epoch 3, Batch: 90, Loss: 0.243353\n",
      "Train - Epoch 3, Batch: 100, Loss: 0.302427\n",
      "Train - Epoch 3, Batch: 110, Loss: 0.363735\n",
      "Train - Epoch 3, Batch: 120, Loss: 0.300138\n",
      "Train - Epoch 3, Batch: 130, Loss: 0.185085\n",
      "Train - Epoch 3, Batch: 140, Loss: 0.275647\n",
      "Train - Epoch 3, Batch: 150, Loss: 0.234474\n",
      "Train - Epoch 3, Batch: 160, Loss: 0.322788\n",
      "Train - Epoch 3, Batch: 170, Loss: 0.247661\n",
      "Train - Epoch 3, Batch: 180, Loss: 0.285024\n",
      "Train - Epoch 3, Batch: 190, Loss: 0.200732\n",
      "Train - Epoch 3, Batch: 200, Loss: 0.198007\n",
      "Train - Epoch 3, Batch: 210, Loss: 0.200615\n",
      "Train - Epoch 3, Batch: 220, Loss: 0.298508\n",
      "Train - Epoch 3, Batch: 230, Loss: 0.210754\n",
      "Train - Epoch 3, Batch: 240, Loss: 0.194263\n",
      "Train - Epoch 3, Batch: 250, Loss: 0.152793\n",
      "Train - Epoch 3, Batch: 260, Loss: 0.228954\n",
      "Test Avg. Loss: 0.000792, Accuracy: 0.944464\n",
      "lenet_epoch=3_test_acc=0.944.pth\n",
      "training...\n",
      "Train - Epoch 4, Batch: 0, Loss: 0.241325\n",
      "Train - Epoch 4, Batch: 10, Loss: 0.206047\n",
      "Train - Epoch 4, Batch: 20, Loss: 0.196890\n",
      "Train - Epoch 4, Batch: 30, Loss: 0.134248\n",
      "Train - Epoch 4, Batch: 40, Loss: 0.275691\n",
      "Train - Epoch 4, Batch: 50, Loss: 0.216294\n",
      "Train - Epoch 4, Batch: 60, Loss: 0.188042\n",
      "Train - Epoch 4, Batch: 70, Loss: 0.184391\n",
      "Train - Epoch 4, Batch: 80, Loss: 0.171354\n",
      "Train - Epoch 4, Batch: 90, Loss: 0.207701\n",
      "Train - Epoch 4, Batch: 100, Loss: 0.193960\n",
      "Train - Epoch 4, Batch: 110, Loss: 0.217334\n",
      "Train - Epoch 4, Batch: 120, Loss: 0.170497\n",
      "Train - Epoch 4, Batch: 130, Loss: 0.175482\n",
      "Train - Epoch 4, Batch: 140, Loss: 0.173156\n",
      "Train - Epoch 4, Batch: 150, Loss: 0.309349\n",
      "Train - Epoch 4, Batch: 160, Loss: 0.124042\n",
      "Train - Epoch 4, Batch: 170, Loss: 0.212658\n",
      "Train - Epoch 4, Batch: 180, Loss: 0.181475\n",
      "Train - Epoch 4, Batch: 190, Loss: 0.261557\n",
      "Train - Epoch 4, Batch: 200, Loss: 0.156682\n",
      "Train - Epoch 4, Batch: 210, Loss: 0.184842\n",
      "Train - Epoch 4, Batch: 220, Loss: 0.214741\n",
      "Train - Epoch 4, Batch: 230, Loss: 0.200978\n",
      "Train - Epoch 4, Batch: 240, Loss: 0.152395\n",
      "Train - Epoch 4, Batch: 250, Loss: 0.184297\n",
      "Train - Epoch 4, Batch: 260, Loss: 0.201977\n",
      "Test Avg. Loss: 0.000604, Accuracy: 0.956183\n",
      "lenet_epoch=4_test_acc=0.956.pth\n",
      "training...\n",
      "Train - Epoch 5, Batch: 0, Loss: 0.150505\n",
      "Train - Epoch 5, Batch: 10, Loss: 0.168571\n",
      "Train - Epoch 5, Batch: 20, Loss: 0.143369\n",
      "Train - Epoch 5, Batch: 30, Loss: 0.180616\n",
      "Train - Epoch 5, Batch: 40, Loss: 0.170596\n",
      "Train - Epoch 5, Batch: 50, Loss: 0.122365\n",
      "Train - Epoch 5, Batch: 60, Loss: 0.184323\n",
      "Train - Epoch 5, Batch: 70, Loss: 0.235358\n",
      "Train - Epoch 5, Batch: 80, Loss: 0.160627\n",
      "Train - Epoch 5, Batch: 90, Loss: 0.148524\n",
      "Train - Epoch 5, Batch: 100, Loss: 0.153865\n",
      "Train - Epoch 5, Batch: 110, Loss: 0.106364\n",
      "Train - Epoch 5, Batch: 120, Loss: 0.207281\n",
      "Train - Epoch 5, Batch: 130, Loss: 0.185471\n",
      "Train - Epoch 5, Batch: 140, Loss: 0.138686\n",
      "Train - Epoch 5, Batch: 150, Loss: 0.154062\n",
      "Train - Epoch 5, Batch: 160, Loss: 0.111273\n",
      "Train - Epoch 5, Batch: 170, Loss: 0.125120\n",
      "Train - Epoch 5, Batch: 180, Loss: 0.122970\n",
      "Train - Epoch 5, Batch: 190, Loss: 0.171408\n",
      "Train - Epoch 5, Batch: 200, Loss: 0.173036\n",
      "Train - Epoch 5, Batch: 210, Loss: 0.158417\n",
      "Train - Epoch 5, Batch: 220, Loss: 0.087282\n",
      "Train - Epoch 5, Batch: 230, Loss: 0.164928\n",
      "Train - Epoch 5, Batch: 240, Loss: 0.112206\n",
      "Train - Epoch 5, Batch: 250, Loss: 0.139816\n",
      "Train - Epoch 5, Batch: 260, Loss: 0.172335\n",
      "Test Avg. Loss: 0.000490, Accuracy: 0.963705\n",
      "lenet_epoch=5_test_acc=0.964.pth\n",
      "training...\n",
      "Train - Epoch 6, Batch: 0, Loss: 0.098816\n",
      "Train - Epoch 6, Batch: 10, Loss: 0.182903\n",
      "Train - Epoch 6, Batch: 20, Loss: 0.101307\n",
      "Train - Epoch 6, Batch: 30, Loss: 0.133030\n",
      "Train - Epoch 6, Batch: 40, Loss: 0.111260\n",
      "Train - Epoch 6, Batch: 50, Loss: 0.200299\n",
      "Train - Epoch 6, Batch: 60, Loss: 0.122865\n",
      "Train - Epoch 6, Batch: 70, Loss: 0.209602\n",
      "Train - Epoch 6, Batch: 80, Loss: 0.113729\n",
      "Train - Epoch 6, Batch: 90, Loss: 0.072835\n",
      "Train - Epoch 6, Batch: 100, Loss: 0.152142\n",
      "Train - Epoch 6, Batch: 110, Loss: 0.073122\n",
      "Train - Epoch 6, Batch: 120, Loss: 0.137475\n",
      "Train - Epoch 6, Batch: 130, Loss: 0.106962\n",
      "Train - Epoch 6, Batch: 140, Loss: 0.103820\n",
      "Train - Epoch 6, Batch: 150, Loss: 0.221151\n",
      "Train - Epoch 6, Batch: 160, Loss: 0.173684\n",
      "Train - Epoch 6, Batch: 170, Loss: 0.082211\n",
      "Train - Epoch 6, Batch: 180, Loss: 0.104077\n",
      "Train - Epoch 6, Batch: 190, Loss: 0.166593\n",
      "Train - Epoch 6, Batch: 200, Loss: 0.088781\n",
      "Train - Epoch 6, Batch: 210, Loss: 0.106723\n",
      "Train - Epoch 6, Batch: 220, Loss: 0.140581\n",
      "Train - Epoch 6, Batch: 230, Loss: 0.108605\n",
      "Train - Epoch 6, Batch: 240, Loss: 0.123742\n",
      "Train - Epoch 6, Batch: 250, Loss: 0.121300\n",
      "Train - Epoch 6, Batch: 260, Loss: 0.135823\n",
      "Test Avg. Loss: 0.000423, Accuracy: 0.968777\n",
      "lenet_epoch=6_test_acc=0.969.pth\n",
      "training...\n",
      "Train - Epoch 7, Batch: 0, Loss: 0.144581\n",
      "Train - Epoch 7, Batch: 10, Loss: 0.134738\n",
      "Train - Epoch 7, Batch: 20, Loss: 0.116171\n",
      "Train - Epoch 7, Batch: 30, Loss: 0.164587\n",
      "Train - Epoch 7, Batch: 40, Loss: 0.112481\n",
      "Train - Epoch 7, Batch: 50, Loss: 0.161774\n",
      "Train - Epoch 7, Batch: 60, Loss: 0.137935\n",
      "Train - Epoch 7, Batch: 70, Loss: 0.174663\n",
      "Train - Epoch 7, Batch: 80, Loss: 0.121910\n",
      "Train - Epoch 7, Batch: 90, Loss: 0.113307\n",
      "Train - Epoch 7, Batch: 100, Loss: 0.103051\n",
      "Train - Epoch 7, Batch: 110, Loss: 0.094654\n",
      "Train - Epoch 7, Batch: 120, Loss: 0.167123\n",
      "Train - Epoch 7, Batch: 130, Loss: 0.104671\n",
      "Train - Epoch 7, Batch: 140, Loss: 0.092370\n",
      "Train - Epoch 7, Batch: 150, Loss: 0.137853\n",
      "Train - Epoch 7, Batch: 160, Loss: 0.110982\n",
      "Train - Epoch 7, Batch: 170, Loss: 0.138192\n",
      "Train - Epoch 7, Batch: 180, Loss: 0.134922\n",
      "Train - Epoch 7, Batch: 190, Loss: 0.142598\n",
      "Train - Epoch 7, Batch: 200, Loss: 0.103400\n",
      "Train - Epoch 7, Batch: 210, Loss: 0.087718\n",
      "Train - Epoch 7, Batch: 220, Loss: 0.107987\n",
      "Train - Epoch 7, Batch: 230, Loss: 0.074446\n",
      "Train - Epoch 7, Batch: 240, Loss: 0.082233\n",
      "Train - Epoch 7, Batch: 250, Loss: 0.108696\n",
      "Train - Epoch 7, Batch: 260, Loss: 0.107585\n",
      "Test Avg. Loss: 0.000376, Accuracy: 0.971226\n",
      "lenet_epoch=7_test_acc=0.971.pth\n",
      "training...\n",
      "Train - Epoch 8, Batch: 0, Loss: 0.133403\n",
      "Train - Epoch 8, Batch: 10, Loss: 0.134099\n",
      "Train - Epoch 8, Batch: 20, Loss: 0.097830\n",
      "Train - Epoch 8, Batch: 30, Loss: 0.102464\n",
      "Train - Epoch 8, Batch: 40, Loss: 0.102376\n",
      "Train - Epoch 8, Batch: 50, Loss: 0.087310\n",
      "Train - Epoch 8, Batch: 60, Loss: 0.164881\n",
      "Train - Epoch 8, Batch: 70, Loss: 0.094954\n",
      "Train - Epoch 8, Batch: 80, Loss: 0.095004\n",
      "Train - Epoch 8, Batch: 90, Loss: 0.101244\n",
      "Train - Epoch 8, Batch: 100, Loss: 0.071505\n",
      "Train - Epoch 8, Batch: 110, Loss: 0.130325\n",
      "Train - Epoch 8, Batch: 120, Loss: 0.119272\n",
      "Train - Epoch 8, Batch: 130, Loss: 0.077008\n",
      "Train - Epoch 8, Batch: 140, Loss: 0.087131\n",
      "Train - Epoch 8, Batch: 150, Loss: 0.110831\n",
      "Train - Epoch 8, Batch: 160, Loss: 0.110494\n",
      "Train - Epoch 8, Batch: 170, Loss: 0.066580\n",
      "Train - Epoch 8, Batch: 180, Loss: 0.077124\n",
      "Train - Epoch 8, Batch: 190, Loss: 0.091055\n",
      "Train - Epoch 8, Batch: 200, Loss: 0.082249\n",
      "Train - Epoch 8, Batch: 210, Loss: 0.094919\n",
      "Train - Epoch 8, Batch: 220, Loss: 0.091891\n",
      "Train - Epoch 8, Batch: 230, Loss: 0.109041\n",
      "Train - Epoch 8, Batch: 240, Loss: 0.097075\n",
      "Train - Epoch 8, Batch: 250, Loss: 0.123553\n",
      "Train - Epoch 8, Batch: 260, Loss: 0.078912\n",
      "Test Avg. Loss: 0.000347, Accuracy: 0.972451\n",
      "lenet_epoch=8_test_acc=0.972.pth\n",
      "training...\n",
      "Train - Epoch 9, Batch: 0, Loss: 0.120228\n",
      "Train - Epoch 9, Batch: 10, Loss: 0.119886\n",
      "Train - Epoch 9, Batch: 20, Loss: 0.067603\n",
      "Train - Epoch 9, Batch: 30, Loss: 0.088189\n",
      "Train - Epoch 9, Batch: 40, Loss: 0.128983\n",
      "Train - Epoch 9, Batch: 50, Loss: 0.109955\n",
      "Train - Epoch 9, Batch: 60, Loss: 0.067462\n",
      "Train - Epoch 9, Batch: 70, Loss: 0.099124\n",
      "Train - Epoch 9, Batch: 80, Loss: 0.067496\n",
      "Train - Epoch 9, Batch: 90, Loss: 0.113951\n",
      "Train - Epoch 9, Batch: 100, Loss: 0.135750\n",
      "Train - Epoch 9, Batch: 110, Loss: 0.145923\n",
      "Train - Epoch 9, Batch: 120, Loss: 0.112852\n",
      "Train - Epoch 9, Batch: 130, Loss: 0.038176\n",
      "Train - Epoch 9, Batch: 140, Loss: 0.070177\n",
      "Train - Epoch 9, Batch: 150, Loss: 0.104960\n",
      "Train - Epoch 9, Batch: 160, Loss: 0.139597\n",
      "Train - Epoch 9, Batch: 170, Loss: 0.093878\n",
      "Train - Epoch 9, Batch: 180, Loss: 0.065939\n",
      "Train - Epoch 9, Batch: 190, Loss: 0.105743\n",
      "Train - Epoch 9, Batch: 200, Loss: 0.051315\n",
      "Train - Epoch 9, Batch: 210, Loss: 0.159678\n",
      "Train - Epoch 9, Batch: 220, Loss: 0.123564\n",
      "Train - Epoch 9, Batch: 230, Loss: 0.037342\n",
      "Train - Epoch 9, Batch: 240, Loss: 0.080554\n",
      "Train - Epoch 9, Batch: 250, Loss: 0.046383\n",
      "Train - Epoch 9, Batch: 260, Loss: 0.111873\n",
      "Test Avg. Loss: 0.000306, Accuracy: 0.975774\n",
      "lenet_epoch=9_test_acc=0.976.pth\n",
      "training...\n",
      "Train - Epoch 10, Batch: 0, Loss: 0.059606\n",
      "Train - Epoch 10, Batch: 10, Loss: 0.165446\n",
      "Train - Epoch 10, Batch: 20, Loss: 0.083733\n",
      "Train - Epoch 10, Batch: 30, Loss: 0.053066\n",
      "Train - Epoch 10, Batch: 40, Loss: 0.071365\n",
      "Train - Epoch 10, Batch: 50, Loss: 0.108485\n",
      "Train - Epoch 10, Batch: 60, Loss: 0.071217\n",
      "Train - Epoch 10, Batch: 70, Loss: 0.075815\n",
      "Train - Epoch 10, Batch: 80, Loss: 0.043937\n",
      "Train - Epoch 10, Batch: 90, Loss: 0.058071\n",
      "Train - Epoch 10, Batch: 100, Loss: 0.123048\n",
      "Train - Epoch 10, Batch: 110, Loss: 0.082615\n",
      "Train - Epoch 10, Batch: 120, Loss: 0.060780\n",
      "Train - Epoch 10, Batch: 130, Loss: 0.094026\n",
      "Train - Epoch 10, Batch: 140, Loss: 0.040978\n",
      "Train - Epoch 10, Batch: 150, Loss: 0.082374\n",
      "Train - Epoch 10, Batch: 160, Loss: 0.061909\n",
      "Train - Epoch 10, Batch: 170, Loss: 0.060885\n",
      "Train - Epoch 10, Batch: 180, Loss: 0.122692\n",
      "Train - Epoch 10, Batch: 190, Loss: 0.112480\n",
      "Train - Epoch 10, Batch: 200, Loss: 0.093897\n",
      "Train - Epoch 10, Batch: 210, Loss: 0.081620\n",
      "Train - Epoch 10, Batch: 220, Loss: 0.074590\n",
      "Train - Epoch 10, Batch: 230, Loss: 0.067115\n",
      "Train - Epoch 10, Batch: 240, Loss: 0.080542\n",
      "Train - Epoch 10, Batch: 250, Loss: 0.123429\n",
      "Train - Epoch 10, Batch: 260, Loss: 0.040994\n",
      "Test Avg. Loss: 0.000287, Accuracy: 0.977348\n",
      "lenet_epoch=10_test_acc=0.977.pth\n",
      "training...\n",
      "Train - Epoch 11, Batch: 0, Loss: 0.079779\n",
      "Train - Epoch 11, Batch: 10, Loss: 0.073368\n",
      "Train - Epoch 11, Batch: 20, Loss: 0.080985\n",
      "Train - Epoch 11, Batch: 30, Loss: 0.112286\n",
      "Train - Epoch 11, Batch: 40, Loss: 0.111364\n",
      "Train - Epoch 11, Batch: 50, Loss: 0.121127\n",
      "Train - Epoch 11, Batch: 60, Loss: 0.042372\n",
      "Train - Epoch 11, Batch: 70, Loss: 0.083806\n",
      "Train - Epoch 11, Batch: 80, Loss: 0.078151\n",
      "Train - Epoch 11, Batch: 90, Loss: 0.079185\n",
      "Train - Epoch 11, Batch: 100, Loss: 0.081210\n",
      "Train - Epoch 11, Batch: 110, Loss: 0.102539\n",
      "Train - Epoch 11, Batch: 120, Loss: 0.119235\n",
      "Train - Epoch 11, Batch: 130, Loss: 0.032868\n",
      "Train - Epoch 11, Batch: 140, Loss: 0.066183\n",
      "Train - Epoch 11, Batch: 150, Loss: 0.085930\n",
      "Train - Epoch 11, Batch: 160, Loss: 0.041642\n",
      "Train - Epoch 11, Batch: 170, Loss: 0.056487\n",
      "Train - Epoch 11, Batch: 180, Loss: 0.136358\n",
      "Train - Epoch 11, Batch: 190, Loss: 0.053554\n",
      "Train - Epoch 11, Batch: 200, Loss: 0.103897\n",
      "Train - Epoch 11, Batch: 210, Loss: 0.083281\n",
      "Train - Epoch 11, Batch: 220, Loss: 0.055927\n",
      "Train - Epoch 11, Batch: 230, Loss: 0.051755\n",
      "Train - Epoch 11, Batch: 240, Loss: 0.062847\n",
      "Train - Epoch 11, Batch: 250, Loss: 0.090156\n",
      "Train - Epoch 11, Batch: 260, Loss: 0.072780\n",
      "Test Avg. Loss: 0.000271, Accuracy: 0.977960\n",
      "lenet_epoch=11_test_acc=0.978.pth\n",
      "training...\n",
      "Train - Epoch 12, Batch: 0, Loss: 0.034876\n",
      "Train - Epoch 12, Batch: 10, Loss: 0.077684\n",
      "Train - Epoch 12, Batch: 20, Loss: 0.091487\n",
      "Train - Epoch 12, Batch: 30, Loss: 0.067649\n",
      "Train - Epoch 12, Batch: 40, Loss: 0.062793\n",
      "Train - Epoch 12, Batch: 50, Loss: 0.055509\n",
      "Train - Epoch 12, Batch: 60, Loss: 0.075124\n",
      "Train - Epoch 12, Batch: 70, Loss: 0.102635\n",
      "Train - Epoch 12, Batch: 80, Loss: 0.087677\n",
      "Train - Epoch 12, Batch: 90, Loss: 0.050350\n",
      "Train - Epoch 12, Batch: 100, Loss: 0.040507\n",
      "Train - Epoch 12, Batch: 110, Loss: 0.069736\n",
      "Train - Epoch 12, Batch: 120, Loss: 0.052545\n",
      "Train - Epoch 12, Batch: 130, Loss: 0.062407\n",
      "Train - Epoch 12, Batch: 140, Loss: 0.099959\n",
      "Train - Epoch 12, Batch: 150, Loss: 0.069634\n",
      "Train - Epoch 12, Batch: 160, Loss: 0.095000\n",
      "Train - Epoch 12, Batch: 170, Loss: 0.082132\n",
      "Train - Epoch 12, Batch: 180, Loss: 0.080300\n",
      "Train - Epoch 12, Batch: 190, Loss: 0.042874\n",
      "Train - Epoch 12, Batch: 200, Loss: 0.081089\n",
      "Train - Epoch 12, Batch: 210, Loss: 0.079736\n",
      "Train - Epoch 12, Batch: 220, Loss: 0.031937\n",
      "Train - Epoch 12, Batch: 230, Loss: 0.066170\n",
      "Train - Epoch 12, Batch: 240, Loss: 0.137401\n",
      "Train - Epoch 12, Batch: 250, Loss: 0.061985\n",
      "Train - Epoch 12, Batch: 260, Loss: 0.070773\n",
      "Test Avg. Loss: 0.000248, Accuracy: 0.979885\n",
      "lenet_epoch=12_test_acc=0.980.pth\n",
      "training...\n",
      "Train - Epoch 13, Batch: 0, Loss: 0.047558\n",
      "Train - Epoch 13, Batch: 10, Loss: 0.082326\n",
      "Train - Epoch 13, Batch: 20, Loss: 0.087512\n",
      "Train - Epoch 13, Batch: 30, Loss: 0.142304\n",
      "Train - Epoch 13, Batch: 40, Loss: 0.103733\n",
      "Train - Epoch 13, Batch: 50, Loss: 0.039478\n",
      "Train - Epoch 13, Batch: 60, Loss: 0.100753\n",
      "Train - Epoch 13, Batch: 70, Loss: 0.138600\n",
      "Train - Epoch 13, Batch: 80, Loss: 0.093214\n",
      "Train - Epoch 13, Batch: 90, Loss: 0.063999\n",
      "Train - Epoch 13, Batch: 100, Loss: 0.066599\n",
      "Train - Epoch 13, Batch: 110, Loss: 0.097404\n",
      "Train - Epoch 13, Batch: 120, Loss: 0.071026\n",
      "Train - Epoch 13, Batch: 130, Loss: 0.080002\n",
      "Train - Epoch 13, Batch: 140, Loss: 0.111958\n",
      "Train - Epoch 13, Batch: 150, Loss: 0.080433\n",
      "Train - Epoch 13, Batch: 160, Loss: 0.029991\n",
      "Train - Epoch 13, Batch: 170, Loss: 0.070958\n",
      "Train - Epoch 13, Batch: 180, Loss: 0.101766\n",
      "Train - Epoch 13, Batch: 190, Loss: 0.078338\n",
      "Train - Epoch 13, Batch: 200, Loss: 0.051774\n",
      "Train - Epoch 13, Batch: 210, Loss: 0.030664\n",
      "Train - Epoch 13, Batch: 220, Loss: 0.073674\n",
      "Train - Epoch 13, Batch: 230, Loss: 0.091715\n",
      "Train - Epoch 13, Batch: 240, Loss: 0.051197\n",
      "Train - Epoch 13, Batch: 250, Loss: 0.116399\n",
      "Train - Epoch 13, Batch: 260, Loss: 0.033433\n",
      "Test Avg. Loss: 0.000237, Accuracy: 0.981284\n",
      "lenet_epoch=13_test_acc=0.981.pth\n",
      "training...\n",
      "Train - Epoch 14, Batch: 0, Loss: 0.085935\n",
      "Train - Epoch 14, Batch: 10, Loss: 0.039439\n",
      "Train - Epoch 14, Batch: 20, Loss: 0.060803\n",
      "Train - Epoch 14, Batch: 30, Loss: 0.037439\n",
      "Train - Epoch 14, Batch: 40, Loss: 0.043346\n",
      "Train - Epoch 14, Batch: 50, Loss: 0.101903\n",
      "Train - Epoch 14, Batch: 60, Loss: 0.051557\n",
      "Train - Epoch 14, Batch: 70, Loss: 0.096227\n",
      "Train - Epoch 14, Batch: 80, Loss: 0.096030\n",
      "Train - Epoch 14, Batch: 90, Loss: 0.064874\n",
      "Train - Epoch 14, Batch: 100, Loss: 0.091501\n",
      "Train - Epoch 14, Batch: 110, Loss: 0.077615\n",
      "Train - Epoch 14, Batch: 120, Loss: 0.056873\n",
      "Train - Epoch 14, Batch: 130, Loss: 0.061953\n",
      "Train - Epoch 14, Batch: 140, Loss: 0.056363\n",
      "Train - Epoch 14, Batch: 150, Loss: 0.092383\n",
      "Train - Epoch 14, Batch: 160, Loss: 0.084344\n",
      "Train - Epoch 14, Batch: 170, Loss: 0.036680\n",
      "Train - Epoch 14, Batch: 180, Loss: 0.051818\n",
      "Train - Epoch 14, Batch: 190, Loss: 0.076843\n",
      "Train - Epoch 14, Batch: 200, Loss: 0.135652\n",
      "Train - Epoch 14, Batch: 210, Loss: 0.064171\n",
      "Train - Epoch 14, Batch: 220, Loss: 0.080490\n",
      "Train - Epoch 14, Batch: 230, Loss: 0.047313\n",
      "Train - Epoch 14, Batch: 240, Loss: 0.053481\n",
      "Train - Epoch 14, Batch: 250, Loss: 0.056102\n",
      "Train - Epoch 14, Batch: 260, Loss: 0.070316\n",
      "Test Avg. Loss: 0.000234, Accuracy: 0.980759\n",
      "lenet_epoch=14_test_acc=0.981.pth\n",
      "training...\n",
      "Train - Epoch 15, Batch: 0, Loss: 0.074108\n",
      "Train - Epoch 15, Batch: 10, Loss: 0.054449\n",
      "Train - Epoch 15, Batch: 20, Loss: 0.043665\n",
      "Train - Epoch 15, Batch: 30, Loss: 0.069097\n",
      "Train - Epoch 15, Batch: 40, Loss: 0.032868\n",
      "Train - Epoch 15, Batch: 50, Loss: 0.114685\n",
      "Train - Epoch 15, Batch: 60, Loss: 0.079574\n",
      "Train - Epoch 15, Batch: 70, Loss: 0.065073\n",
      "Train - Epoch 15, Batch: 80, Loss: 0.074523\n",
      "Train - Epoch 15, Batch: 90, Loss: 0.033154\n",
      "Train - Epoch 15, Batch: 100, Loss: 0.073431\n",
      "Train - Epoch 15, Batch: 110, Loss: 0.039191\n",
      "Train - Epoch 15, Batch: 120, Loss: 0.080333\n",
      "Train - Epoch 15, Batch: 130, Loss: 0.116103\n",
      "Train - Epoch 15, Batch: 140, Loss: 0.067591\n",
      "Train - Epoch 15, Batch: 150, Loss: 0.020741\n",
      "Train - Epoch 15, Batch: 160, Loss: 0.052284\n",
      "Train - Epoch 15, Batch: 170, Loss: 0.103146\n",
      "Train - Epoch 15, Batch: 180, Loss: 0.050192\n",
      "Train - Epoch 15, Batch: 190, Loss: 0.081457\n",
      "Train - Epoch 15, Batch: 200, Loss: 0.046359\n",
      "Train - Epoch 15, Batch: 210, Loss: 0.047679\n",
      "Train - Epoch 15, Batch: 220, Loss: 0.121247\n",
      "Train - Epoch 15, Batch: 230, Loss: 0.069423\n",
      "Train - Epoch 15, Batch: 240, Loss: 0.109645\n",
      "Train - Epoch 15, Batch: 250, Loss: 0.064279\n",
      "Train - Epoch 15, Batch: 260, Loss: 0.043752\n",
      "Test Avg. Loss: 0.000219, Accuracy: 0.982683\n",
      "lenet_epoch=15_test_acc=0.983.pth\n",
      "training...\n",
      "Train - Epoch 16, Batch: 0, Loss: 0.053381\n",
      "Train - Epoch 16, Batch: 10, Loss: 0.038751\n",
      "Train - Epoch 16, Batch: 20, Loss: 0.051979\n",
      "Train - Epoch 16, Batch: 30, Loss: 0.053950\n",
      "Train - Epoch 16, Batch: 40, Loss: 0.053196\n",
      "Train - Epoch 16, Batch: 50, Loss: 0.040108\n",
      "Train - Epoch 16, Batch: 60, Loss: 0.032169\n",
      "Train - Epoch 16, Batch: 70, Loss: 0.069623\n",
      "Train - Epoch 16, Batch: 80, Loss: 0.051104\n",
      "Train - Epoch 16, Batch: 90, Loss: 0.046441\n",
      "Train - Epoch 16, Batch: 100, Loss: 0.069781\n",
      "Train - Epoch 16, Batch: 110, Loss: 0.057969\n",
      "Train - Epoch 16, Batch: 120, Loss: 0.099795\n",
      "Train - Epoch 16, Batch: 130, Loss: 0.069277\n",
      "Train - Epoch 16, Batch: 140, Loss: 0.061936\n",
      "Train - Epoch 16, Batch: 150, Loss: 0.063928\n",
      "Train - Epoch 16, Batch: 160, Loss: 0.055205\n",
      "Train - Epoch 16, Batch: 170, Loss: 0.066225\n",
      "Train - Epoch 16, Batch: 180, Loss: 0.084605\n",
      "Train - Epoch 16, Batch: 190, Loss: 0.057346\n",
      "Train - Epoch 16, Batch: 200, Loss: 0.049561\n",
      "Train - Epoch 16, Batch: 210, Loss: 0.065279\n",
      "Train - Epoch 16, Batch: 220, Loss: 0.070212\n",
      "Train - Epoch 16, Batch: 230, Loss: 0.075733\n",
      "Train - Epoch 16, Batch: 240, Loss: 0.029138\n",
      "Train - Epoch 16, Batch: 250, Loss: 0.087503\n",
      "Train - Epoch 16, Batch: 260, Loss: 0.062432\n",
      "Test Avg. Loss: 0.000211, Accuracy: 0.982683\n",
      "lenet_epoch=16_test_acc=0.983.pth\n",
      "training...\n",
      "Train - Epoch 17, Batch: 0, Loss: 0.053214\n",
      "Train - Epoch 17, Batch: 10, Loss: 0.015926\n",
      "Train - Epoch 17, Batch: 20, Loss: 0.055175\n",
      "Train - Epoch 17, Batch: 30, Loss: 0.079188\n",
      "Train - Epoch 17, Batch: 40, Loss: 0.091802\n",
      "Train - Epoch 17, Batch: 50, Loss: 0.034531\n",
      "Train - Epoch 17, Batch: 60, Loss: 0.048193\n",
      "Train - Epoch 17, Batch: 70, Loss: 0.041855\n",
      "Train - Epoch 17, Batch: 80, Loss: 0.050979\n",
      "Train - Epoch 17, Batch: 90, Loss: 0.042941\n",
      "Train - Epoch 17, Batch: 100, Loss: 0.080376\n",
      "Train - Epoch 17, Batch: 110, Loss: 0.075223\n",
      "Train - Epoch 17, Batch: 120, Loss: 0.105037\n",
      "Train - Epoch 17, Batch: 130, Loss: 0.072027\n",
      "Train - Epoch 17, Batch: 140, Loss: 0.072010\n",
      "Train - Epoch 17, Batch: 150, Loss: 0.097346\n",
      "Train - Epoch 17, Batch: 160, Loss: 0.026043\n",
      "Train - Epoch 17, Batch: 170, Loss: 0.079579\n",
      "Train - Epoch 17, Batch: 180, Loss: 0.055029\n",
      "Train - Epoch 17, Batch: 190, Loss: 0.039932\n",
      "Train - Epoch 17, Batch: 200, Loss: 0.044313\n",
      "Train - Epoch 17, Batch: 210, Loss: 0.074282\n",
      "Train - Epoch 17, Batch: 220, Loss: 0.045499\n",
      "Train - Epoch 17, Batch: 230, Loss: 0.044436\n",
      "Train - Epoch 17, Batch: 240, Loss: 0.070401\n",
      "Train - Epoch 17, Batch: 250, Loss: 0.051347\n",
      "Train - Epoch 17, Batch: 260, Loss: 0.036276\n",
      "Test Avg. Loss: 0.000200, Accuracy: 0.983558\n",
      "lenet_epoch=17_test_acc=0.984.pth\n",
      "training...\n",
      "Train - Epoch 18, Batch: 0, Loss: 0.081361\n",
      "Train - Epoch 18, Batch: 10, Loss: 0.081651\n",
      "Train - Epoch 18, Batch: 20, Loss: 0.050926\n",
      "Train - Epoch 18, Batch: 30, Loss: 0.063588\n",
      "Train - Epoch 18, Batch: 40, Loss: 0.044415\n",
      "Train - Epoch 18, Batch: 50, Loss: 0.043515\n",
      "Train - Epoch 18, Batch: 60, Loss: 0.060618\n",
      "Train - Epoch 18, Batch: 70, Loss: 0.070978\n",
      "Train - Epoch 18, Batch: 80, Loss: 0.045476\n",
      "Train - Epoch 18, Batch: 90, Loss: 0.072059\n",
      "Train - Epoch 18, Batch: 100, Loss: 0.033297\n",
      "Train - Epoch 18, Batch: 110, Loss: 0.041954\n",
      "Train - Epoch 18, Batch: 120, Loss: 0.038573\n",
      "Train - Epoch 18, Batch: 130, Loss: 0.031395\n",
      "Train - Epoch 18, Batch: 140, Loss: 0.100931\n",
      "Train - Epoch 18, Batch: 150, Loss: 0.077945\n",
      "Train - Epoch 18, Batch: 160, Loss: 0.047469\n",
      "Train - Epoch 18, Batch: 170, Loss: 0.063034\n",
      "Train - Epoch 18, Batch: 180, Loss: 0.044687\n",
      "Train - Epoch 18, Batch: 190, Loss: 0.023424\n",
      "Train - Epoch 18, Batch: 200, Loss: 0.042232\n",
      "Train - Epoch 18, Batch: 210, Loss: 0.071295\n",
      "Train - Epoch 18, Batch: 220, Loss: 0.083241\n",
      "Train - Epoch 18, Batch: 230, Loss: 0.087650\n",
      "Train - Epoch 18, Batch: 240, Loss: 0.077227\n",
      "Train - Epoch 18, Batch: 250, Loss: 0.045972\n",
      "Train - Epoch 18, Batch: 260, Loss: 0.029019\n",
      "Test Avg. Loss: 0.000204, Accuracy: 0.984520\n",
      "lenet_epoch=18_test_acc=0.985.pth\n",
      "training...\n",
      "Train - Epoch 19, Batch: 0, Loss: 0.096375\n",
      "Train - Epoch 19, Batch: 10, Loss: 0.029019\n",
      "Train - Epoch 19, Batch: 20, Loss: 0.029849\n",
      "Train - Epoch 19, Batch: 30, Loss: 0.096452\n",
      "Train - Epoch 19, Batch: 40, Loss: 0.070467\n",
      "Train - Epoch 19, Batch: 50, Loss: 0.055091\n",
      "Train - Epoch 19, Batch: 60, Loss: 0.046226\n",
      "Train - Epoch 19, Batch: 70, Loss: 0.028727\n",
      "Train - Epoch 19, Batch: 80, Loss: 0.058126\n",
      "Train - Epoch 19, Batch: 90, Loss: 0.045298\n",
      "Train - Epoch 19, Batch: 100, Loss: 0.045748\n",
      "Train - Epoch 19, Batch: 110, Loss: 0.051753\n",
      "Train - Epoch 19, Batch: 120, Loss: 0.062320\n",
      "Train - Epoch 19, Batch: 130, Loss: 0.060493\n",
      "Train - Epoch 19, Batch: 140, Loss: 0.057840\n",
      "Train - Epoch 19, Batch: 150, Loss: 0.024561\n",
      "Train - Epoch 19, Batch: 160, Loss: 0.102381\n",
      "Train - Epoch 19, Batch: 170, Loss: 0.035132\n",
      "Train - Epoch 19, Batch: 180, Loss: 0.020483\n",
      "Train - Epoch 19, Batch: 190, Loss: 0.060728\n",
      "Train - Epoch 19, Batch: 200, Loss: 0.035844\n",
      "Train - Epoch 19, Batch: 210, Loss: 0.052626\n",
      "Train - Epoch 19, Batch: 220, Loss: 0.032199\n",
      "Train - Epoch 19, Batch: 230, Loss: 0.042770\n",
      "Train - Epoch 19, Batch: 240, Loss: 0.057374\n",
      "Train - Epoch 19, Batch: 250, Loss: 0.032754\n",
      "Train - Epoch 19, Batch: 260, Loss: 0.042976\n",
      "Test Avg. Loss: 0.000187, Accuracy: 0.985045\n",
      "lenet_epoch=19_test_acc=0.985.pth\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, 20):\n",
    "    acc = train_and_test(e)\n",
    "    #if e % 2 == 0:\n",
    "    #    torch.save(net.state_dict(), f'autodl-tmp/save_model/mnist/Lenet1/re_lenet_epoch={e}_test_acc={acc:0.3f}.pth')\n",
    "    print(f'lenet_epoch={e}_test_acc={acc:0.3f}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906261c7-b7fe-4bac-83e0-3bb75a8922cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
